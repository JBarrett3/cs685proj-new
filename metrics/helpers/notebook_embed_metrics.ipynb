{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9300e",
   "metadata": {},
   "source": [
    "### Plotting Functions\n",
    "Three different functions to visually display the embeddings1. First item\n",
    "1. ``plot_similarity_heatmap``: Displays pairwise cosine similarity between all words as a heatmap. Visually shows intra- and inter-group similarities.\n",
    "2. ``plot_embeddings_2d``: Projects the embedding onto 2D scatterplot to visualize the group clusters.\n",
    "3. ``plot_similarity_distributions``: Compares the distribution of intra-group and out-group similarities using a violin plot.\n",
    "\n",
    "Currently shows the plots since that is nice in notebooks, but we could change to savefig depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc8295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Heat map of the cosine similarity matrix. Ideally groups should have high similarity. \n",
    "def plot_similarity_heatmap(word_grid, sim_matrix):\n",
    "    words = word_grid.flatten()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        sim_matrix,\n",
    "        xticklabels=words,\n",
    "        yticklabels=words,\n",
    "        cmap=\"BuPu\",\n",
    "        square=True,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={\"label\": \"Cosine Similarity\"}\n",
    "    )\n",
    "    plt.title(\"Pairwise Cosine Similarity of Word Embeddings\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plots the embeddings on 2D after reducing the dimensionality. Ideally the groups should be close together, but in 2D hard to say.\n",
    "def plot_embeddings_2d(word_grid, embeddings, method='tsne'):\n",
    "    words = word_grid.flatten()\n",
    "    flat_embeddings = embeddings.reshape(-1, embeddings.shape[-1])\n",
    "\n",
    "    # Dimensionality reduction\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'pca' or 'tsne'\")\n",
    "\n",
    "    reduced = reducer.fit_transform(flat_embeddings)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    for i in range(4):\n",
    "        idxs = list(range(i*4, (i+1)*4))\n",
    "        plt.scatter(reduced[idxs, 0], reduced[idxs, 1], label=f\"Group {i+1}\", color=colors[i])\n",
    "        for idx in idxs:\n",
    "            plt.text(reduced[idx, 0], reduced[idx, 1], words[idx], fontsize=9)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(f\"Word Embeddings 2D Projection ({method.upper()})\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compares the distribution of in group and out group similarities. Generally out group are much more spread out.\n",
    "def plot_similarity_distributions(intra_group_sims, out_group_sims):\n",
    "    data = (\n",
    "        [(\"Intra-group\", sim) for sim in intra_group_sims] +\n",
    "        [(\"Out-group\", sim) for sim in out_group_sims]\n",
    "    )\n",
    "    labels, sims = zip(*data)\n",
    "    sns.violinplot(x=labels, y=sims, inner=\"box\", palette=\"pastel\")\n",
    "    plt.title(\"Distribution of Cosine Similarities\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511e22e",
   "metadata": {},
   "source": [
    "### Metrics function\n",
    "This function outputs the average intra-group similarity, the intra-group similarity for each group, the average out-group similarity, and a proposed grouping based on the similarities. \n",
    "\n",
    "By default does not display the plots, can use for tracking metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6cfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_group_metrics(model, tokenizer, word_grid, plot = False):\n",
    "    embeddings = np.zeros((4, 4, model.config.hidden_size))\n",
    "\n",
    "    # Generate Embeddings with model\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            word = word_grid[i, j]\n",
    "            embeddings[i, j] = get_word_embedding(model, tokenizer, word)\n",
    "\n",
    "    flat_embeddings = embeddings.reshape(-1, embeddings.shape[-1])  # (16, hidden_dim)\n",
    "    sim_matrix = cosine_similarity(flat_embeddings)\n",
    "\n",
    "    # Intra-group similarity\n",
    "    intra_group_sims = []\n",
    "    for group_idx in range(4):\n",
    "        idxs = [group_idx * 4 + i for i in range(4)]\n",
    "        group_sims = sim_matrix[np.ix_(idxs, idxs)]\n",
    "        upper_triangle = group_sims[np.triu_indices(4, k=1)]\n",
    "        intra_group_sims.append(upper_triangle.mean())\n",
    "\n",
    "    # Out-group similarity\n",
    "    out_group_sims = []\n",
    "    for i in range(16):\n",
    "        group_i = i // 4\n",
    "        for j in range(16):\n",
    "            group_j = j // 4\n",
    "            if group_i != group_j:\n",
    "                out_group_sims.append(sim_matrix[i, j])\n",
    "\n",
    "    if plot:\n",
    "        plot_embeddings_2d(word_grid, embeddings)\n",
    "        plot_similarity_heatmap(word_grid, sim_matrix)\n",
    "        plot_similarity_distributions(intra_group_sims, out_group_sims)\n",
    "\n",
    "    used = set()\n",
    "    groups = []\n",
    "\n",
    "    # Greedy grouping of the words based on similarity\n",
    "    all_indices = list(range(16))\n",
    "    while len(used) < 16:\n",
    "        best_group = None\n",
    "        best_score = -float('inf')\n",
    "        for combo in itertools.combinations([i for i in all_indices if i not in used], 4):\n",
    "            score = sum(sim_matrix[i][j] for i, j in itertools.combinations(combo, 2))\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_group = combo\n",
    "        groups.append(best_group)\n",
    "        used.update(best_group)\n",
    "\n",
    "    words_flat = word_grid.flatten()\n",
    "\n",
    "    return {\n",
    "        \"avg_intra_group_similarity\": np.mean(intra_group_sims),\n",
    "        \"intra_group_similarities\": intra_group_sims,\n",
    "        \"avg_out_group_similarity\": np.mean(out_group_sims),\n",
    "        \"group_assignments\": [[words_flat[idx] for idx in group] for group in groups],\n",
    "    }\n",
    "\n",
    "def get_word_embedding(model, tokenizer, word):\n",
    "    # Tokenize the word\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Get the hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Choose the last hidden layer\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "    # Each token has its own vector; average across tokens if the word was split\n",
    "    word_embedding = last_hidden_state[0].mean(dim=0)  # shape: (hidden_dim,)\n",
    "    return word_embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa12ee",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "Given the correct inputs, the function will output the similarity of the embeddings within groups, out of groupts, and the proposed groupings based on the embeddings. We could do some comparison based on the groupings, but I think that will fluctuate a lot based on how we select groups so I envision that being used for examples.\n",
    "\n",
    "Probably we should plot the average in and out group similarities over the duration of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cef3fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      1\u001b[39m words = [\n\u001b[32m      2\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCITRUS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLEAFY GREENS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSTEADY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m         ]\n\u001b[32m     20\u001b[39m word_grid = np.array(words).reshape((\u001b[32m4\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m metrics = compute_group_metrics(\u001b[43mmodel\u001b[49m, tokenizer, word_grid, plot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAverage Intra-Group Similarity:\u001b[39m\u001b[33m\"\u001b[39m, metrics[\u001b[33m\"\u001b[39m\u001b[33mavg_intra_group_similarity\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, sim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics[\u001b[33m\"\u001b[39m\u001b[33mintra_group_similarities\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "            \"CITRUS\",\n",
    "            \"LEAFY GREENS\",\n",
    "            \"SUNSHINE\",\n",
    "            \"SUPPLEMENTS\",\n",
    "            \"CITY\",\n",
    "            \"LAND\",\n",
    "            \"TOWN\",\n",
    "            \"WORLD\",\n",
    "            \"AMERICAN FLAG\",\n",
    "            \"GALAXY\",\n",
    "            \"RED CARPET\",\n",
    "            \"UBER RATING\",\n",
    "            \"ALL OUT\",\n",
    "            \"BETWEEN\",\n",
    "            \"KART\",\n",
    "            \"STEADY\"\n",
    "        ]\n",
    "\n",
    "word_grid = np.array(words).reshape((4, 4))\n",
    "metrics = compute_group_metrics(model, tokenizer, word_grid, plot=True)\n",
    "\n",
    "print(\"Average Intra-Group Similarity:\", metrics[\"avg_intra_group_similarity\"])\n",
    "for idx, sim in enumerate(metrics[\"intra_group_similarities\"]):\n",
    "    print(f\"Group {idx+1} similarity: {sim:.4f}\")\n",
    "\n",
    "print(\"Average Out-Group Similarity:\", metrics[\"avg_out_group_similarity\"])\n",
    "\n",
    "print(\"Greedy Groupings:\")\n",
    "for i, group in enumerate(metrics[\"group_assignments\"]):\n",
    "    print(f\"Group {i+1}: {group}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
