# NYT Crossword Solver

## Structure

This repository contains four primary folders: `conventional`, `data`, `grpo`, and `metrics`.

### `conventional`
This folder contains all the code relevant to training a standard-SFT (Supervised Fine-Tuning) model. 

- **finetune.ipynb** was used to test and converge on the final script, **finetune.py**, which trains the model. 
- **finetune.sh** is a batch script used to run the training process.
- **inference.py** and **eval.py** handle inference and evaluation on the test set, respectively.
- **plot_loss.py** generates plots of the training and validation loss over time.

Additionally, there is a subfolder named **results**, which contains:
- Inferences,
- Loss plots,
- Embedding metrics for a 100-example sample, the best model, and an untrained model.

Note: The checkpoints are not committed to the repository due to their large size. However, these can be regenerated by running the `finetune` notebook, Python script, or batch script.

### `data`
This folder contains the raw NYT games file (`raw.txt`), which is converted into a JSON format (`originalJSON.json`) by **make_json.py**. The data is then shuffled and saved as `shuffledJSON.json`.

- The raw data is further processed into a dataset format that can be used for the model by **make_ds.py**. 

The generated dataset is not committed to the repository due to its large size, but users can regenerate it using the provided scripts.

### `grpo`
\todo{Michael on GRPO}

### `metrics`
This folder contains the visualization code for evaluating model performance:

- **notebook_embed_metrics.ipynb** is used for initial engineering and visualizing metrics.
- **script_embed_metrics.py** is used to implement the same metrics in a script format.
- **plot_embed_metrics.py** iterates over checkpoints and/or models to generate visualizations for each.