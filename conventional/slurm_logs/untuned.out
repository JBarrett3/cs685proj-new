Removing conda
Loading conda
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
INFO 05-08 10:47:46 [importing.py:53] Triton module has been replaced with a placeholder.
INFO 05-08 10:47:46 [__init__.py:239] Automatically detected platform cuda.
Unsloth 2025.4.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Thu May  8 10:47:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A40                     On  |   00000000:65:00.0 Off |                    0 |
|  0%   34C    P0             79W /  300W |     269MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    785190      C   python                                        260MiB |
+-----------------------------------------------------------------------------------------+

INFO: PyTorch Version: 2.6.0+cu124
INFO: CUDA Version: 12.4
INFO: cuDNN Version: 90100
INFO: CUDA Available: True
INFO: GPU Count: 1
INFO: Training: False, Limit: -1, Epochs: 5, Batch Size: 64, Learning Rate: 0.001
==((====))==  Unsloth 2025.4.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.
   \\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.339 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
INFO: Model loaded
INFO: First 25 of 32 layers frozen. Rest are trainable.
INFO: QLora applied
INFO: Loaded dataset of 690 samples
INFO: Max prompted length:988
INFO: Configured
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:01<00:00,  1.48it/s]100%|██████████| 2/2 [00:01<00:00,  1.40it/s]
cleaned cache post eval

Test Loss: 1.8093949556350708
INFO: Bypassing training
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:01<00:00,  1.08it/s]100%|██████████| 2/2 [00:01<00:00,  1.01it/s]
cleaned cache post eval

Test Loss: 1.8093949556350708
  0%|          | 0/69 [00:00<?, ?it/s]  1%|▏         | 1/69 [00:04<05:32,  4.89s/it]  3%|▎         | 2/69 [00:09<05:19,  4.77s/it]  4%|▍         | 3/69 [00:14<05:12,  4.73s/it]  6%|▌         | 4/69 [00:18<05:06,  4.71s/it]  7%|▋         | 5/69 [00:23<05:01,  4.70s/it]  9%|▊         | 6/69 [00:28<04:56,  4.70s/it] 10%|█         | 7/69 [00:33<04:51,  4.70s/it] 12%|█▏        | 8/69 [00:37<04:46,  4.69s/it] 13%|█▎        | 9/69 [00:42<04:41,  4.69s/it] 14%|█▍        | 10/69 [00:47<04:36,  4.69s/it] 16%|█▌        | 11/69 [00:51<04:32,  4.69s/it] 17%|█▋        | 12/69 [00:56<04:27,  4.69s/it] 19%|█▉        | 13/69 [01:01<04:22,  4.69s/it] 20%|██        | 14/69 [01:05<04:17,  4.69s/it] 22%|██▏       | 15/69 [01:10<04:13,  4.69s/it] 23%|██▎       | 16/69 [01:15<04:08,  4.69s/it] 25%|██▍       | 17/69 [01:19<04:04,  4.69s/it] 26%|██▌       | 18/69 [01:24<03:59,  4.69s/it] 28%|██▊       | 19/69 [01:29<03:54,  4.69s/it] 29%|██▉       | 20/69 [01:34<03:49,  4.69s/it] 30%|███       | 21/69 [01:38<03:45,  4.69s/it] 32%|███▏      | 22/69 [01:43<03:40,  4.69s/it] 33%|███▎      | 23/69 [01:48<03:35,  4.69s/it] 35%|███▍      | 24/69 [01:52<03:31,  4.69s/it] 36%|███▌      | 25/69 [01:57<03:26,  4.70s/it] 38%|███▊      | 26/69 [02:02<03:21,  4.69s/it] 39%|███▉      | 27/69 [02:06<03:17,  4.69s/it] 41%|████      | 28/69 [02:11<03:12,  4.69s/it] 42%|████▏     | 29/69 [02:16<03:07,  4.69s/it] 43%|████▎     | 30/69 [02:20<03:03,  4.69s/it] 45%|████▍     | 31/69 [02:25<02:58,  4.71s/it] 46%|████▋     | 32/69 [02:30<02:54,  4.70s/it] 48%|████▊     | 33/69 [02:35<02:48,  4.69s/it] 49%|████▉     | 34/69 [02:39<02:43,  4.68s/it] 51%|█████     | 35/69 [02:44<02:39,  4.68s/it] 52%|█████▏    | 36/69 [02:49<02:34,  4.68s/it] 54%|█████▎    | 37/69 [02:53<02:29,  4.68s/it] 55%|█████▌    | 38/69 [02:58<02:25,  4.69s/it] 57%|█████▋    | 39/69 [03:03<02:21,  4.71s/it] 58%|█████▊    | 40/69 [03:07<02:16,  4.71s/it] 59%|█████▉    | 41/69 [03:12<02:11,  4.71s/it] 61%|██████    | 42/69 [03:17<02:07,  4.71s/it] 62%|██████▏   | 43/69 [03:22<02:02,  4.71s/it] 64%|██████▍   | 44/69 [03:26<01:57,  4.71s/it] 65%|██████▌   | 45/69 [03:31<01:53,  4.71s/it] 67%|██████▋   | 46/69 [03:36<01:48,  4.71s/it] 68%|██████▊   | 47/69 [03:40<01:43,  4.71s/it] 70%|██████▉   | 48/69 [03:45<01:38,  4.71s/it] 71%|███████   | 49/69 [03:50<01:34,  4.71s/it] 72%|███████▏  | 50/69 [03:55<01:29,  4.71s/it] 74%|███████▍  | 51/69 [03:59<01:24,  4.71s/it] 75%|███████▌  | 52/69 [04:04<01:20,  4.71s/it] 77%|███████▋  | 53/69 [04:09<01:15,  4.71s/it] 78%|███████▊  | 54/69 [04:13<01:10,  4.72s/it] 80%|███████▉  | 55/69 [04:18<01:06,  4.72s/it] 81%|████████  | 56/69 [04:23<01:01,  4.72s/it] 83%|████████▎ | 57/69 [04:28<00:56,  4.71s/it] 84%|████████▍ | 58/69 [04:32<00:51,  4.71s/it] 86%|████████▌ | 59/69 [04:37<00:47,  4.71s/it] 87%|████████▋ | 60/69 [04:42<00:42,  4.71s/it] 88%|████████▊ | 61/69 [04:46<00:37,  4.72s/it] 90%|████████▉ | 62/69 [04:51<00:33,  4.72s/it] 91%|█████████▏| 63/69 [04:56<00:28,  4.72s/it] 93%|█████████▎| 64/69 [05:01<00:23,  4.72s/it] 94%|█████████▍| 65/69 [05:05<00:18,  4.72s/it] 96%|█████████▌| 66/69 [05:10<00:14,  4.72s/it] 97%|█████████▋| 67/69 [05:15<00:09,  4.72s/it] 99%|█████████▊| 68/69 [05:19<00:04,  4.72s/it]100%|██████████| 69/69 [05:24<00:00,  4.72s/it]100%|██████████| 69/69 [05:24<00:00,  4.71s/it]
INFO: Inference complete
