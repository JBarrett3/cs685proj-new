{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22733272",
   "metadata": {},
   "source": [
    "Make sure to be using `unsloth` for environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2030259",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-5\n",
    "LIMIT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1920619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "from vllm import SamplingParams\n",
    "import json\n",
    "from unsloth import is_bfloat16_supported, FastLanguageModel\n",
    "from datasets import Dataset, load_from_disk\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22247c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  4 11:09:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla M40 24GB                 Off |   00000000:02:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             60W /  250W |    6710MiB /  23040MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         2022260      C   ...conda/envs/unsloth/bin/python       6696MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "cuDNN Version: 90100\n",
      "CUDA Available: True\n",
      "GPU Count: 1\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "!nvidia-smi\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2243ba14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.\n",
      "   \\\\   /|    Tesla M40 24GB. Num GPUs = 1. Max memory: 22.395 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 5.2. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# model loading\n",
    "max_seq_length = 1256 # prompts are ~1000, so leaving 256 for response\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.1-8B-Instruct\", # \"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885d68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 25 layers frozen. Rest are trainable.\n"
     ]
    }
   ],
   "source": [
    "# layer freezing\n",
    "total_layers = len(model.model.layers)\n",
    "freeze_percentage = 80\n",
    "num_freeze_layers = int(total_layers * (freeze_percentage / 100))\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    if i < num_freeze_layers:\n",
    "        for param in layer.parameters():\n",
    "            if param.dtype in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "                param.requires_grad = False\n",
    "    else:\n",
    "        for param in layer.parameters():\n",
    "            if param.dtype in [torch.float16, torch.bfloat16, torch.float32]:\n",
    "                param.requires_grad = True\n",
    "print(f\"First {num_freeze_layers} layers frozen. Rest are trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29b5a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# qlora\n",
    "lora_rank = 32\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d3d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset of 10 samples\n"
     ]
    }
   ],
   "source": [
    "# Data prep\n",
    "dataset = load_from_disk(\"../data/connections_ds\")\n",
    "# Note that you will need to run make_ds.py ahead of time to generate this dataset\n",
    "\n",
    "dataset = dataset.map(lambda example: {\"text\": example[\"input\"]}, remove_columns=[\"input\"])\n",
    "dataset = dataset.map(lambda example: {\"label\": example[\"target\"]}, remove_columns=[\"target\"])\n",
    "\n",
    "dataset = dataset.select(range(0, 10))\n",
    "train_test_split = dataset.train_test_split(test_size=0.1) # splits 10% off to test\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "print(f\"Loaded dataset of {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f2554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n"
     ]
    }
   ],
   "source": [
    "# Note prompt lengths\n",
    "prompt_lengths = [tokenizer(example['text'], return_tensors=\"pt\")[\"input_ids\"].shape[1] for example in test_dataset]\n",
    "print(max(prompt_lengths)) # maxing out at most at 1000 tokens in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3702ff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 9/9 [00:01<00:00,  5.83 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 39.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "date = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "class ClearCacheCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\" Clear GPU cache after each epoch \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        return control\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = EPOCHS,\n",
    "    learning_rate = LR,\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = f\"/home/jamesbarrett_umass_edu/cs685proj-new/conventional/outputs/{date}\",\n",
    "    logging_dir = f\"/home/jamesbarrett_umass_edu/cs685proj-new/conventional/model_logs/{date}\",\n",
    "    logging_first_step = True,\n",
    "    logging_strategy = \"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    report_to = \"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = training_args,\n",
    "    callbacks=[ClearCacheCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2217f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9 | Num Epochs = 1 | Total steps = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 4 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 83,886,080/8,000,000,000 (1.05% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.793500</td>\n",
       "      <td>1.795320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tune (if TRAINING)\n",
    "if TRAINING:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "655acc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64160ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.79531991481781\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Test Loss:\", eval_result[\"eval_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc09494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache after evaluation\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1258bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "predictions = []\n",
    "for input in tqdm(test_dataset['text']):\n",
    "    tok_input = tokenizer([input], truncation=True, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "    output = model.generate(input_ids=tok_input['input_ids'][0].unsqueeze(0), attention_mask=tok_input['attention_mask'][0].unsqueeze(0), max_new_tokens = 128, use_cache = True)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    predictions.append({\n",
    "        'input_sentence': input,\n",
    "        'whole prediction': generated_text,\n",
    "        'new token prediction': generated_text.split('Groupings:')[-1]\n",
    "    })\n",
    "\n",
    "with open(f'inferences_training={TRAINING}.json', 'w') as json_file:\n",
    "    json.dump(predictions, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
